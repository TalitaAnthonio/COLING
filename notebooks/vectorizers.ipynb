{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "...     'This is the first document.',\n",
    "...     'This document is the second document.',\n",
    "...     'And this is the third one.',\n",
    "...     'Is this the first document?',\n",
    "... ]\n",
    "labels = [\"Positive\", \"Negative\", \"Negative\", \"Positive\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 8)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 1)\t1\n",
      "  (1, 8)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 6)\t1\n",
      "  (1, 1)\t2\n",
      "  (1, 5)\t1\n",
      "  (2, 8)\t1\n",
      "  (2, 3)\t1\n",
      "  (2, 6)\t1\n",
      "  (2, 0)\t1\n",
      "  (2, 7)\t1\n",
      "  (2, 4)\t1\n",
      "  (3, 8)\t1\n",
      "  (3, 3)\t1\n",
      "  (3, 6)\t1\n",
      "  (3, 2)\t1\n",
      "  (3, 1)\t1\n",
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(X)\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def type_token_ratio(document, regex=False):\n",
    "    \"\"\"\n",
    "        Input: tokenize document \n",
    "        Returns: the type-token-ratio for a document.\n",
    "    \"\"\"\n",
    "    if regex:\n",
    "        all_tokens = regex_tokeniser(document)\n",
    "    else:\n",
    "        print(\"do not use regex\")\n",
    "        all_tokens = word_tokenize(document)\n",
    "    num_of_tokens = len(all_tokens)\n",
    "    unique_tokens = list(set(all_tokens))\n",
    "    num_of_unique_tokens = len(unique_tokens)\n",
    "    print(\"num_of_unique_tokens/num_of_tokens\")\n",
    "    return {\"Type-token-ratio\": num_of_unique_tokens/num_of_tokens}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LexicalComplexity(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def _get_features(self, doc):\n",
    "        lexical_score = type_token_ratio(doc)\n",
    "        print(lexical_score)\n",
    "        return lexical_score\n",
    "    \n",
    "\n",
    "    def transform(self, raw_documents):\n",
    "        return [self._get_features(doc) for doc in raw_documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>context</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>This is a sentence</td>\n",
       "      <td>Context for sent1.</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>this is a sentence.</td>\n",
       "      <td>Context for sent 2 and yeah you know Context.</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  text                                        context  \\\n",
       "0   This is a sentence                             Context for sent1.   \n",
       "1  this is a sentence.  Context for sent 2 and yeah you know Context.   \n",
       "\n",
       "     labels  \n",
       "0  negative  \n",
       "1  positive  "
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "data = {'text': [\"This is a sentence\", \"this is a sentence.\"], 'context': [\"Context for sent1.\", \"Context for sent 2 and yeah you know Context.\"], \"labels\": [\"negative\", \"positive\"] }\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexical_complexity_vec = Pipeline(\n",
    "    [\n",
    "        ('feat', LexicalComplexity()), ('vec', DictVectorizer())\n",
    "    ]\n",
    ")\n",
    "\n",
    "count_vectorizer = TfidfVectorizer()\n",
    "count_vectorizer_2 = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer, make_column_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_transformer = ColumnTransformer(\n",
    "                    transformers=[\n",
    "                        ('vec1', count_vectorizer, 'text'), ('vec2', lexical_complexity_vec, 'context')] ,  remainder='drop',\n",
    "                    n_jobs=-1,\n",
    "                    sparse_threshold=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ColumnTransformer(n_jobs=-1, remainder='drop', sparse_threshold=0,\n",
       "                  transformer_weights=None,\n",
       "                  transformers=[('vec1',\n",
       "                                 TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                                 decode_error='strict',\n",
       "                                                 dtype=<class 'numpy.float64'>,\n",
       "                                                 encoding='utf-8',\n",
       "                                                 input='content',\n",
       "                                                 lowercase=True, max_df=1.0,\n",
       "                                                 max_features=None, min_df=1,\n",
       "                                                 ngram_range=(1, 1), norm='l2',\n",
       "                                                 preprocessor=None,\n",
       "                                                 smooth_idf=True,\n",
       "                                                 stop_words=None,\n",
       "                                                 strip_accents=None,\n",
       "                                                 sublinear_tf=False,\n",
       "                                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                                 tokenizer=None, use_idf=True,\n",
       "                                                 vocabulary=None),\n",
       "                                 'text'),\n",
       "                                ('vec2',\n",
       "                                 Pipeline(memory=None,\n",
       "                                          steps=[('feat', LexicalComplexity()),\n",
       "                                                 ('vec',\n",
       "                                                  DictVectorizer(dtype=<class 'numpy.float64'>,\n",
       "                                                                 separator='=',\n",
       "                                                                 sort=True,\n",
       "                                                                 sparse=True))],\n",
       "                                          verbose=False),\n",
       "                                 'context')],\n",
       "                  verbose=False)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.57735027, 0.57735027, 0.57735027, 1.        ],\n",
       "       [0.57735027, 0.57735027, 0.57735027, 0.9       ]])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain_transformed = col_transformer.fit_transform(df)\n",
    "Xtrain_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "Y = df['labels'].to_list()\n",
    "MultinomialNB().fit(Xtrain_transformed, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do not use regex\n",
      "do not use regex\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<2x4 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 8 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#compare with union \n",
    "\n",
    "lexical_complexity_vec = Pipeline(\n",
    "    [\n",
    "        ('feat', LexicalComplexity()), ('vec', DictVectorizer())\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "ex = [\"This is a sentence\", \"this is a sentence.\"]\n",
    "union = FeatureUnion([(\"vec1\", count_vectorizer), (\"vec2\", lexical_complexity_vec)])\n",
    "union.fit_transform(ex)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do not use regex\n",
      "num_of_unique_tokens/num_of_tokens\n",
      "do not use regex\n",
      "num_of_unique_tokens/num_of_tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<2x1 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 2 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex = lexical_complexity_vec.fit_transform(df['context'])\n",
    "ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['negative', 'positive']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['labels'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['is', 'sentence', 'this']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform([\"This is a sentence\", \"this is a sentence.\"])\n",
    "vectorizer.get_feature_names()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MultinomialNB().fit(X, [1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexical_complexity_vec = Pipeline(\n",
    "    [\n",
    "        ('feat', LexicalComplexity()), ('vec', DictVectorizer())\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = [\"Context for sent1.\", \"Context for sent 2 and yeah you know heh.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do not use regex\n",
      "num_of_unique_tokens/num_of_tokens\n",
      "{'Type-token-ratio': 1.0}\n",
      "do not use regex\n",
      "num_of_unique_tokens/num_of_tokens\n",
      "{'Type-token-ratio': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('feat', LexicalComplexity()),\n",
       "                ('vec',\n",
       "                 DictVectorizer(dtype=<class 'numpy.float64'>, separator='=',\n",
       "                                sort=True, sparse=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexical_complexity_vec.fit(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do not use regex\n",
      "num_of_unique_tokens/num_of_tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Type-token-ratio': 1.0}"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type_token_ratio(c[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "num_of_tokens = len(word_tokenize(c[1]))\n",
    "\n",
    "print(num_of_tokens)\n",
    "#unique_tokens = list(set(all_tokens))\n",
    "#num_of_unique_tokens = len(unique_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Context', 'for', 'sent', '2', 'and', 'yeah', 'you', 'know', 'heh', '.']"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tokens = list(set(word_tokenize(c[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sent', 'you', '2', 'heh', 'and', 'yeah', 'for', 'know', '.', 'Context']"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
